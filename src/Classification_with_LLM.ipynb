{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugging Face Model Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
    "\n",
    "model = HuggingFaceEndpoint(repo_id=model_name, temperature=0.01, max_new_tokens= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"hw4-train-new.csv\")\n",
    "test_data = pd.read_csv(\"hw4-llm-test-part2-new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(true_labels, predictions):\n",
    "    precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(f\"Precision: {precision * 100:.2f}\")\n",
    "    print(f\"Recall: {recall * 100:.2f}\")\n",
    "    print(f\"Macro F1: {f1 * 100:.2f}\")\n",
    "    \n",
    "def rating(response):\n",
    "    # 0 is \"low-star\" or unkown (defaulting to 0, since 0 is \"low-star\"), 1 is \"high-star\"\n",
    "    response = response.strip().lower()\n",
    "    if \"high-star\" in response:\n",
    "        return 1\n",
    "    elif \"low-star\" in response:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0 #default\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# zero-shot classifier function\n",
    "def zero_shot_classifier(review_text, model):\n",
    "    prompt = f\"\"\" Task: Classify review as either 'high-star' or 'low-star'.\n",
    "                Review to classify: \"{review_text}\"\n",
    "            \"\"\"\n",
    "    \n",
    "    \n",
    "    #     prompt = f\"\"\"Classify review as either 'high-star' or 'low-star'.\n",
    "\n",
    "    #     Example:\n",
    "    #     Review: \"{review_sample}\"\n",
    "    #     Classification: {'high-star' if classification_sample == 1 else 'low-star'}\n",
    "\n",
    "    #     Now classify this review:\n",
    "    #     Review: \"{review_text}\"\n",
    "\n",
    "    #     Rules for classification:\n",
    "    #     - high-star or 1: positive sentiment, customer satisfaction\n",
    "    #     - low-star or 0: negative or neutral sentiment, customer dissatisfaction\n",
    "    \n",
    "    # \"\"\"\n",
    "    response = model(prompt)\n",
    "    return (rating(response))\n",
    "\n",
    "\n",
    "pred_zero_shot = []\n",
    "\n",
    "for review_text in test_data['reviewText']:\n",
    "    prediction = zero_shot_classifier(review_text, model)\n",
    "    pred_zero_shot.append(prediction)\n",
    "\n",
    "#zero-shot evaluation\n",
    "print(\"Zero-Shot Evaluation Results:\")\n",
    "evaluate (test_data['label'].tolist(), pred_zero_shot )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_classifier( review_text,model, train_data):\n",
    "    \n",
    "    example = train_data.sample(n= 1)\n",
    "    example_review = example['reviewText'].values[0]\n",
    "    #print(review_sample)\n",
    "    #print(\"\\n\")\n",
    "    example_label = example['label'].values[0]\n",
    "    #print(classification_sample)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Task: Classify the following review as 'high-star' or 'low-star':\\n\"\n",
    "        f\"Example review: {example_review}\\n\"\n",
    "        f\"Example classification: {'high-star' if example_label == 1 else 'low-star'}\\n\\n\"\n",
    "        f\"Now classify the following review: {review_text}\"\n",
    "    )   \n",
    "    \n",
    "    # prompt = f\"\"\" Task: Classify review as either 'high-star' or 'low-star'.\n",
    "\n",
    "    #     This is an example of a review and its classification:\n",
    "    #         Example:\n",
    "    #         Review: \"{review_sample}\"\n",
    "    #         Classification: {'high-star' if classification_sample == 1 else 'low-star'}\n",
    "\n",
    "    #     Now classify this review:\n",
    "    #     Review: \"{review_text}\"\n",
    "\n",
    "    #     Rules for classification:\n",
    "    #     - high-star or 1: positive sentiment, customer satisfaction\n",
    "    #     - low-star or 0: negative or neutral sentiment, customer dissatisfaction\n",
    "    \n",
    "    # \"\"\"\n",
    "    \n",
    "    response = model(prompt)\n",
    "\n",
    "    return rating(response)\n",
    "\n",
    "\n",
    "pred_one_shot =[]\n",
    "for review_text in test_data['reviewText']:\n",
    "    prediction = one_shot_classifier(review_text, model, train_data)\n",
    "    pred_one_shot.append(prediction)\n",
    "\n",
    "\n",
    "#one_shot evaluation\n",
    "print(\"One-Shot Evaluation Results:\") \n",
    "\n",
    "evaluate(test_data['label'].tolist() , pred_one_shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_classifier(review_text, model, train_data):\n",
    "    # Randomly sample n examples from the training data for few-shot context\n",
    "    examples = train_data.sample(n=7)\n",
    "    \n",
    "#     examples = [\n",
    "#     {'reviewText': \"This product is amazing! It exceeded my expectations in every way.\", 'label': 1},  # high-star\n",
    "#     {'reviewText': \"Great value for the price. The performance is top-notch, I highly recommend it.\", 'label': 1},  # high-star\n",
    "#     {'reviewText': \"Worst purchase ever. The item arrived damaged and customer service was unhelpful.\", 'label': 0},  # low-star\n",
    "# ]\n",
    "\n",
    "    prompt = \"Classify the following review as 'high-star' or 'low-star':\\n\"\n",
    "    prompt += \"Here are a few examples (does not include the review to classify):\\n\\n\"\n",
    "\n",
    "\n",
    "    # for example in examples:\n",
    "    #     review = example['reviewText']\n",
    "    #     label = example['label']\n",
    "    #     classification = 'high-star' if label == 1 else 'low-star'\n",
    "    #     prompt += f\"Example Review: {review}\\n\"\n",
    "    #     prompt += f\"Example Classification: {classification}\\n\"\n",
    "\n",
    "\n",
    "    for review, label in zip(examples['reviewText'], examples['label']):\n",
    "        classification = 'high-star' if label == 1 else 'low-star'\n",
    "        prompt += f\"Example Review: {review} -> Classification: {classification}\\n\"\n",
    "\n",
    "    # Add the new review for classification\n",
    "    prompt += f\"Now classify this review: \\n Review: {review_text} -> Classification: ?\"\n",
    "    \n",
    "    response = model(prompt)\n",
    "    return rating(response)\n",
    "\n",
    "pred_few_shot = []\n",
    "\n",
    "for review_text in test_data['reviewText']:\n",
    "    prediction = few_shot_classifier(review_text, model, train_data)\n",
    "    pred_few_shot.append(prediction)\n",
    "\n",
    "# Few-shot evaluation\n",
    "print(\"Few-Shot Evaluation Results:\")\n",
    "evaluate(test_data['label'].tolist(), pred_few_shot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
